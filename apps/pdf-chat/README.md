# PDF Chat Application

Write here everything about your application.

## Setup

For working locally, please add a file named `.env.local` where we can place our environment variables. This file is not committed to Git, therefore it is safe to store sensitive information in it.

After starting Supabase, copy the service role key from the Supabase project settings and add it to the `.env.local` file.

```
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
```

## Stripe

For the Stripe integration, first we need to start the Stripe CLI:

```
pnpm run stripe:listen
```

Then, update the `.env.local` file with the following variables:

```
STRIPE_WEBHOOK_SECRET=
STRIPE_SECRET_KEY=
NEXT_PUBLIC_STRIPE_PUBLISHABLE_KEY=
```

### Stripe Webhook

Please also add the following webhook to your Stripe account:

```
- invoice.paid
```

We need to listen to the `invoice.paid` event to update the `credits_usage` table in the DB.

## LLM

Please add the following LLM (OpenAI Compatible) keys:

```
LLM_API_KEY=*****************************************
LLM_MODEL_NAME=gpt-3.5-turbo
```

If you are using a different URL to access the LLM, add the following environment variable:

```
LLM_BASE_URL=
```

The above uses the `gpt-3.5-turbo` model. You can change it to any other model you have access to - as long as it is OpenAI compatible.

#### Vector Dimension

You will need to update the Supabase database to use the correct vector dimension for your model. To do so, update the migration `supabase/migrations/20240106212957_pdf-chat-schema.sql` or create a new one with the required changes.

There are two instances where you need to update the vector dimension:

1. When defining the `embedding` column in the `documents` table
2. When defining the `query_embedding` type in the `match_documents` function

You will use the same vector dimension for both instances.

#### Indexing Chunk Size

By default, we index embeddings using chunks with length 1500. This is to make sure we don't hit the maximum context length.

If you use models that allow indexing a larger chunk size, you can tweak the settings using the following environment variable:

```
DOCUMENT_CHUNK_SIZE=4000
```

### Supabase

Please follow the instructions in the [Supabase README](../supabase/README.md) to setup your Supabase project.

### Plans

#### Adding a Plan to the Database

You are free to specify your own limitations in the DB.

To add a plan, you will insert a new row in the `plans` table. The `plans` table has the following information:

```sql
create table plans (
  name text not null,
  variant_id text not null,
  tokens_quota bigint not null,
  primary key (variant_id)
);
```

1. The `tokens_quota` is the number of tokens the user can use per month.

The `variant_id` is the ID of the line item. You can find it in the Stripe dashboard or Lemon Squeezy.

We also create the `credits_usage` table, which is used to track the usage of the accounts:

```sql
create table credits_usage (
  id bigint generated by default as identity primary key,
  account_id uuid not null references public.accounts on delete cascade,
  tokens_quota bigint default 500000 not null
);
```

This table is automatically generated when the organization is created. By default - we allow 500,000 tokens per month. The tokens will be incremented when the user pays an invoice (which we track using Stripe's Webhooks).

### When are the tokens counted?

1. The tokens are decremented when the AI responds to a message
2. The tokens are decremented when the user uploads a document

##### Non-Subscribed Users / Free Plan

Non-subscribed users are given a one-off quota of 500,000 tokens. This is to allow them to test the application. Once they reach the limit, they need to subscribe to a plan.

If you wish to implement quota resets for free plan users, you can ask them to subscribe to a 0$ plan, and add the plan details to the `plans` table in the DB.

This is so you can listen to the Stripe Webhooks and update the `organization_usage` table accordingly. Alternatively, you'll need some way to reset their usage monthly - which is not implemented in this template.

## How does it work?

### Indexing PDFs

We use `unpdf` to extract the text from the PDFs. The text is then indexed using Langchain and an Embeddings model of your choice (defaults to OpenAI).

**NB**: the `unpdf` library is the best way I found for converting PDFs to text. However, it's not perfect - and not all PDFs will be able to be parsed. If you find a better way, please let me know.

### Chat

The chats are fetched from the `conversations` table.

The conversation is fetched from the DB the first time it's requested, and then cached in the browser using `swr`. The messages streamed from the LLM are then added to the conversation cache.
